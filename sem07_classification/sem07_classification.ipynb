{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Семинар 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Теория"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/esokolov/ml-minor-hse/blob/master/colloquium-2017/colloquium_minor_problems_linear.pdf\n",
    "\n",
    "Задачи 2, 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Логистическая регрессия\n",
    "\n",
    "Рассмотрим искусственный датасет:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(13)\n",
    "\n",
    "n = 50\n",
    "X = np.random.normal(size=(n, 2))\n",
    "y = np.zeros(n)\n",
    "X[25:] += 3\n",
    "y[:25] -= 1\n",
    "y[25:] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='winter', s=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что классы линейно разделимы. Но прежде чем идти дальше, давайте рассмотрим логистическую функцию:\n",
    "\n",
    "$$ \\sigma(x) = \\frac{1}{1+e^{-x}}$$\n",
    "\n",
    "Посмотрим, как она трансформирует $x \\in [-5, 5]$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "x = np.arange(-5, 6)\n",
    "plt.hlines(y=0.5, xmin=-5, xmax=5, label='y=0.5', color='b')\n",
    "plt.plot(x, sigmoid(x))\n",
    "plt.title(\"Sigmoid(x)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Выходит, что сигмоида переводит любое числовое значение в интервал [0, 1].\n",
    "\n",
    "* В задаче бинарной классификации мы можем рассматривать результат такого преобразования как вероятность принадлежности к положительному классу.\n",
    "\n",
    "* Если значение сигмоиды больше `0.5`, то можем считать, что предсказали класс `1`, если меньше, то класс `-1`.\n",
    "\n",
    "---\n",
    "\n",
    "Итак, **Логистическая регрессия = сигмоида(Линейная регрессия)**\n",
    "\n",
    "Что это значит?\n",
    "\n",
    "У логистической регрессии будет ровно столько же параметров, сколько и у линейной.\n",
    "\n",
    "И выглядит это вот так:\n",
    "\n",
    "$$\n",
    "\\begin{array}{c}\n",
    "a(x) = w_0 + w_1 x_1 + w_2 x_2 \\\\\n",
    "p(y = 1 | x) = \\sigma(a(x)) = \\frac{1}{1 + e^{-a(x)}}\\\\\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Градиентный спуск для логистической регрессии\n",
    "\n",
    "1) Задача оптимизации :\n",
    "\n",
    "$$\n",
    "L(w) = \\frac{1}{\\ell}\\sum\\limits_{i=1}^\\ell\\log_2\\left(1 + e^{-y_i\\langle w, x_i\\rangle}\\right) \\rightarrow \\min_w\n",
    "$$\n",
    "\n",
    "2) Выведите формулу для шага градиентного спуска:\n",
    "\n",
    "$$\n",
    "w_T = w_{T - 1} - \\eta \\frac{\\partial L(w_{T - 1})}{\\partial w_{T - 1}} = ...\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.c_[np.ones(n), X]\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grad(y, X, w):\n",
    "    res = 0\n",
    "    for i in range(len(X)):\n",
    "        res +=   # ̿'̿'\\̵͇̿̿\\з=( ͠° ͟ʖ ͡°)=ε/̵͇̿̿/'̿̿ ̿ ̿ ̿ ̿ ̿\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# предсказание для одного объекта\n",
    "def prediction(w, x):\n",
    "    return sigmoid(w @ x)\n",
    "\n",
    "\n",
    "# предсказание для всего датасета\n",
    "def prediction_dataset(w, X):\n",
    "    preds = []\n",
    "    for i in range(len(X)):\n",
    "        pred = prediction(w, X[i])\n",
    "        if pred >= 0.5:\n",
    "            preds.append(1)\n",
    "        else:\n",
    "            preds.append(-1)\n",
    "    return np.array(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1000\n",
    "eta = 0.02\n",
    "\n",
    "w = np.zeros(3)\n",
    "\n",
    "accuracy_list = []\n",
    "\n",
    "for _ in range(N):\n",
    "    w -= eta * grad(y, X, w)\n",
    "    accuracy_list.append(accuracy_score(y, prediction_dataset(w, X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(accuracy_list)\n",
    "plt.title(\"Accuracy\")\n",
    "plt.xlabel(\"Iteration number\")\n",
    "plt.ylabel(\"Accuracy value\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_min, x_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "y_min, y_max = X[:, 2].min() - .5, X[:, 2].max() + .5\n",
    "h = .02\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "Z = prediction_dataset(w, np.c_[np.ones_like(xx.ravel()), xx.ravel(), yy.ravel()])>0.5\n",
    "\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.pcolormesh(xx, yy, Z, cmap=plt.cm.Paired)\n",
    "\n",
    "plt.scatter(X[:, 1], X[:, 2], c=y, edgecolors='k', s=110)\n",
    "plt.xlabel('X1')\n",
    "plt.ylabel('X2')\n",
    "\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Полиномиальные признаки\n",
    "\n",
    "**XOR проблема**\n",
    "\n",
    "XOR – это \"исключающее ИЛИ\", булева функция со следующей таблицей истинности:\n",
    "\n",
    "<img src=\"https://habrastorage.org/storage2/9af/1de/e09/9af1dee09d4d36ff0b15bdb4aae19e3b.png\" width=400 align=\"center\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(13)\n",
    "X = np.random.randn(200, 2)\n",
    "y = np.logical_xor(X[:, 0] > 0, X[:, 1] > 0)\n",
    "plt.scatter(X[:, 0], X[:, 1], s=30, c=y, cmap=plt.cm.Paired)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_boundary(clf, X, y, plot_title):\n",
    "    xx, yy = np.meshgrid(np.linspace(-3, 3, 50),\n",
    "                     np.linspace(-3, 3, 50))\n",
    "    clf.fit(X, y)\n",
    "    Z = clf.predict_proba(np.vstack((xx.ravel(), yy.ravel())).T)[:, 1]\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    image = plt.imshow(Z, interpolation='nearest',\n",
    "                           extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n",
    "                           aspect='auto', origin='lower', cmap=plt.cm.PuOr_r)\n",
    "    contours = plt.contour(xx, yy, Z, levels=[0], linewidths=2,\n",
    "                               linetypes='--')\n",
    "    plt.scatter(X[:, 0], X[:, 1], s=30, c=y, cmap=plt.cm.Paired)\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "    plt.xlabel(r'$x_1$')\n",
    "    plt.ylabel(r'$x_2$')\n",
    "    plt.axis([-3, 3, -3, 3])\n",
    "    plt.colorbar(image)\n",
    "    plt.title(plot_title, fontsize=12);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Применяем логистическую регрессию к нашим данным\n",
    "# И видим, что линейная модель не способна разделить точки правильно\n",
    "plot_boundary(LogisticRegression(), X, y, \"Logistic Regression, XOR problem\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Что мы делаем?\n",
    "Добавляем полиномиальные признаки!\n",
    "\n",
    "Т.е. получаем несколько признаков из данных путем возведения их в степень и перемножения между собой.\n",
    "\n",
    "**Пример с двумерными данными, degree=2**\n",
    "\n",
    "$$\\Large (a, b) \\rightarrow (1, a, b, a^2, ab, b^2)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "logit_pipe = Pipeline([('poly', PolynomialFeatures(degree=2)), \n",
    "                       ('logit', LogisticRegression())])\n",
    "plot_boundary(logit_pipe, X, y,\n",
    "        \"Logistic Regression + quadratic features. XOR problem\")\n",
    "\n",
    "# Проблема решается!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
