{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Семинар 10-11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (14, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задача.** Пусть решается задача регрессии на одномерной выборке $X = \\{ (x_i, y_i)\\}_{i=1}^l,$ при этом истинная зависимость целевой переменной является линейной: $y(x) = ax + \\varepsilon, \\, \\varepsilon \\sim p(\\varepsilon) = \\mathcal{N} (0, \\sigma^2).$ Допустим, не зная этого, вы обучили на выборке линейную регрессию и решающее дерево с функционалом MSE, и вам известно, что модели не переобучились. После этого вы получили новые данные и построили на них прогнозы обеих моделей, и оказалось, что для решающего дерева значение функционала ошибки на новых данных оказалось радикально выше, чем для линейной регрессии. Чем это может быть вызвано?\n",
    "\n",
    "**Решение.**\n",
    "Поскольку истинная зависимость в данных является линейной, логично предположить, что линейная модель покажет лучшие результаты на подобной выборке. Опишем формально ситуацию, в которой у решающего дерева могут возникнуть серьезные проблемы с восстановлением истинной зависимости.\n",
    "\n",
    "Допустим, обучающая выборка была получена из отрезка $[0; 10],$ обучим соответствующие модели и построим прогнозы для этого отрезка:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_size = 100\n",
    "lin_coef = 3\n",
    "sigma = 1\n",
    "\n",
    "np.random.seed(13)\n",
    "\n",
    "X_train = (np.random.rand(set_size) * 10).reshape(-1, 1)\n",
    "Y_train = X_train * 3 + sigma * np.random.randn(set_size).reshape(-1, 1)\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.scatter(X_train, Y_train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, Y_train)\n",
    "tree = DecisionTreeRegressor()\n",
    "tree.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = np.arange(-1, 12, 0.1).reshape(-1, 1)\n",
    "\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.scatter(X_train, Y_train)\n",
    "plt.plot(grid, lin_coef * grid, 'magenta')\n",
    "plt.plot(grid, lr.predict(grid), 'red',)\n",
    "plt.plot(grid, tree.predict(grid), 'green')\n",
    "plt.xlim([-1, 11])\n",
    "plt.legend(['Ground truth', 'Linear regression', 'Decision tree'], loc=0)\n",
    "print ('LR train MSE = ', mean_squared_error(Y_train, lr.predict(X_train)))\n",
    "print ('DT train MSE = ', mean_squared_error(Y_train, tree.predict(X_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предположим, что новые данные были получены из другой области пространства ответов, например, из отрезка $[20; 30].$ В этом случае предсказания линейной регрессии окажутся гораздо ближе к правде, что отразится и на значении функционала ошибки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = np.arange(-1, 32, 0.1).reshape(-1, 1)\n",
    "\n",
    "X_test = (20 + np.random.rand(set_size) * 10).reshape(-1, 1)\n",
    "Y_test = X_test * 3 + sigma * np.random.randn(set_size).reshape(-1, 1)\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.scatter(X_train, Y_train, c='blue')\n",
    "plt.scatter(X_test, Y_test, c='red')\n",
    "\n",
    "plt.plot(grid, lin_coef * grid, 'magenta')\n",
    "plt.plot(grid, lr.predict(grid), 'red',)\n",
    "plt.plot(grid, tree.predict(grid), 'green')\n",
    "plt.xlim([-1, 31])\n",
    "plt.legend(['Ground truth', 'Linear regression', 'Decision tree'], loc=0)\n",
    "print ('LR test MSE = ', mean_squared_error(Y_test, lr.predict(X_test)))\n",
    "print ('DT test MSE = ', mean_squared_error(Y_test, tree.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вывод**: решающие деревья (а потому и композиции над ними) непригодны для экстраполяции функций."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Решающее дерево регрессии"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сгенерируем искусственный датасет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(13)\n",
    "\n",
    "n = 202\n",
    "X = np.linspace(-2, 2, n).reshape(-1, 1)\n",
    "X = np.c_[X, X ** 2, X ** 3]\n",
    "y = np.sin(X[:, 0] ** 4) + np.random.random(size=n) / 2\n",
    "y = y.ravel()\n",
    "pd.DataFrame({'x': X[:, 0], 'y': y}).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добейтесь наименьшего MSE для регрессора ниже. Какая максимальная глубина ему соответствует?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X[:, 0], y, label='true function')\n",
    "plt.title('Function estimation based on DT')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "\n",
    "# train and predict a regression tree\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "clf = DecisionTreeRegressor(max_depth=1)\n",
    "clf.fit(X, y)\n",
    "y_pred = clf.predict(X)\n",
    "\n",
    "plt.scatter(X[:, 0], y_pred, label='tree prediction', color='orange')\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "\n",
    "print('Mean Squared Error: ', mean_squared_error(y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь давайте найдем оптимальную глубину с помощью отложенной выборки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=13)\n",
    "\n",
    "clf = DecisionTreeRegressor(max_depth=5)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_train)\n",
    "print('train MSE: ', mean_squared_error(y_train, clf.predict(X_train)))\n",
    "print('test MSE: ', mean_squared_error(y_test, clf.predict(X_test)))\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(X_train[:, 0], y_train, label='true function')\n",
    "plt.scatter(X_train[:, 0], clf.predict(X_train), label='tree prediction', color='orange')\n",
    "plt.title('Function estimation based on DT: train')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend(loc='best')\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(X_test[:, 0], y_test, label='true function')\n",
    "plt.scatter(X_test[:, 0], clf.predict(X_test), label='tree prediction', color='orange')\n",
    "plt.title('Function estimation based on DT: test')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Бэггинг"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В дереве регрессий мы ограничивали максимальную глубину дерева для предотвращения переобучения. Как еще можно бороться с переобучением? Давайте попробуем метод бэггинга, позволяющий использовать композицию деревьев, обученных на различных подвыборках обучающей выборки. Ответ - усредненное значение ответов для каждого дерева. Таким образом, обучая деревья на различных подвыборках, мы стараемся сделать их менее скоррелированными.\n",
    "\n",
    "Подберите оптимальное значение максимальной глубины дерева для метода бэггинга. Попробуйте также менять параметр `n_estimators` - за что он отвечает? Нужно ли менять другие гиперпараметры?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingRegressor\n",
    "\n",
    "clf = BaggingRegressor(DecisionTreeRegressor(max_depth=1), n_estimators=10, random_state=13)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_train)\n",
    "print('train MSE: ', mean_squared_error(y_train, clf.predict(X_train)))\n",
    "print('test MSE: ', mean_squared_error(y_test, clf.predict(X_test)))\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(X_train[:, 0], y_train, label='true function')\n",
    "plt.scatter(X_train[:, 0], clf.predict(X_train), label='tree prediction', color='orange')\n",
    "plt.title('Function estimation based on bagging: train')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend(loc='best')\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(X_test[:, 0], y_test, label='true function')\n",
    "plt.scatter(X_test[:, 0], clf.predict(X_test), label='tree prediction', color='orange')\n",
    "plt.title('Function estimation based on bagging: test')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видно, ответы, полученные с помощью бэггинга, меньше выстраиваются в кусочно-постоянные области. Почему так происходит? Как это может помочь в решении задачи?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Случайный лес"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn-images-1.medium.com/max/1560/1*10t9S7xvWE5Z3NEZrmHG2w.jpeg\" width=700>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как можно сделать деревья еще менее скоррелированными? Обучать каждое дерево в ансамбле не только на случайной подвыборке, но и на случайном подмножестве признаков.\n",
    "\n",
    "Подберите оптимальные значения параметров случайного леса."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "clf = RandomForestRegressor(n_estimators=10, max_depth=1, max_features=None, random_state=13)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_train)\n",
    "print('train MSE: ', mean_squared_error(y_train, clf.predict(X_train)))\n",
    "print('test MSE: ', mean_squared_error(y_test, clf.predict(X_test)))\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(X_train[:, 0], y_train, label='true function')\n",
    "plt.scatter(X_train[:, 0], clf.predict(X_train), label='tree prediction', color='orange')\n",
    "plt.title('Function estimation based on RF: train')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend(loc='best')\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(X_test[:, 0], y_test, label='true function')\n",
    "plt.scatter(X_test[:, 0], clf.predict(X_test), label='tree prediction', color='orange')\n",
    "plt.title('Function estimation based on RF: test')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Градиентный бустинг"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Идея бустинга - будем обучать алгоритмы в композиции не независимо, а последовательно, так, чтобы каждый следующий старался исправить ошибки построенной композиции. Начальное предсказание можно просто инициализировать константой. Далее, пусть $N - 1$ алгоритмов в композиции уже построены:\n",
    "\n",
    "$$\n",
    "a_{N - 1}(x) = \\sum\\limits_{n = 1}^{N - 1}b_n(x)\n",
    "$$\n",
    "\n",
    "Следующий алгоритм $b_N(x)$ получается из следующей задачи оптимизации:\n",
    "\n",
    "$$\n",
    "\\frac{1}{\\ell}\\sum\\limits_{i=1}^{\\ell}\\left(b_N(x_i) - (y_i - a_{N - 1}(x_i))\\right)^2 \\to \\min\\limits_{b_N}\n",
    "$$\n",
    "\n",
    "Таким образом, если удастся решить эту задачу идеально, то для каждого объекта $x_i$ будет верно:\n",
    "\n",
    "$$\n",
    "b_N(x_i) - (y_i - a_{N - 1}(x_i)) = 0 \\Rightarrow y_i = a_{N - 1}(x_i) + b_N(x_i) = a_N(x_i)\n",
    "$$\n",
    "\n",
    "**Общий вид.** Пусть $L$ - функция потерь. Тогда общая задача оптимизации выглядит следующим образом:\n",
    "\n",
    "$$\n",
    "\\frac{1}{\\ell}\\sum\\limits_{i=1}^{\\ell}L\\left(y_i, a(x_i)\\right) \\to \\min_a,\n",
    "$$\n",
    "\n",
    "где $a$ - это взвешенная комбинация базовых моделей:\n",
    "\n",
    "$$\n",
    "a(x_i) = \\sum\\limits_n\\gamma_nb_n(x_i)\n",
    "$$\n",
    "\n",
    "Далее, пусть $N - 1$ алгоритмов в композиции уже построены:\n",
    "\n",
    "$$\n",
    "a_{N - 1}(x) = \\sum\\limits_{n = 1}^{N - 1}\\gamma_nb_n(x)\n",
    "$$\n",
    "\n",
    "Следующий алгоритм $b_N(x)$ получается из следующей задачи оптимизации:\n",
    "\n",
    "$$\n",
    "\\frac{1}{\\ell}\\sum\\limits_{i=1}^{\\ell}L\\left(y_i, a_{N - 1}(x_i) + \\gamma_Nb_N(x_i)\\right) \\to \\min\\limits_{\\gamma_N, b_N}\n",
    "$$\n",
    "\n",
    "Таким образом, для каждого $x_i$ мы хотим найти $b_N(x_i)$, минимизирующий $L\\left(y_i, a_{N - 1}(x_i) + b_N(x_i)\\right)$, а также оптимальную длину шага. Для этого давайте проделаем что-то вроде градиентного спуска в пространстве алгоритмов:\n",
    "\n",
    "$$\n",
    "b_N(x_i) = -\\frac{\\partial L(y_i, z)}{\\partial z}\\Bigl|_{z = a_{N - 1}(x_i)}\n",
    "$$\n",
    "\n",
    "Если подставить это значение в задачу оптимизации, то можно заметить, что мы действительно \"сдвинули\" значение функции потерь в сторону наискорейшего убывания по $a_{N - 1}(x_i)$. Следовательно, градиентный бустинг можно рассматривать как процесс градиентного спуска в пространстве алгоритмов.\n",
    "\n",
    "Итак, очередной базовый алгоритм $b_N(x)$ можно обучить предсказывать $-\\frac{\\partial L(y_i, z)}{\\partial z}\\Bigl|_{z = a_{N - 1}(x)}$. В свою очередь, после этого уже можно получить оптимальный шаг:\n",
    "\n",
    "$$\n",
    "\\gamma_N = \\mathrm{arg}\\min_\\gamma \\frac{1}{\\ell}\\sum\\limits_{i=1}^{\\ell}L\\left(y_i, a_{N - 1}(x_i) + \\gamma b_N(x_i)\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вопрос.** Если градиентный бустинг решает задачу классификации, то какую задачу решает каждый из его базовых алгоритмов?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подберите оптимальные значения параметров градиентного бустинга."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "clf = GradientBoostingRegressor(max_depth=1, learning_rate=1e-12, n_estimators=1, random_state=13)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_train)\n",
    "print('train MSE: ', mean_squared_error(y_train, clf.predict(X_train)))\n",
    "print('test MSE: ', mean_squared_error(y_test, clf.predict(X_test)))\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(X_train[:, 0], y_train, label='true function')\n",
    "plt.scatter(X_train[:, 0], clf.predict(X_train), label='tree prediction', color='orange')\n",
    "plt.title('Function estimation based on GB: train')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend(loc='best')\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(X_test[:, 0], y_test, label='true function')\n",
    "plt.scatter(X_test[:, 0], clf.predict(X_test), label='tree prediction', color='orange')\n",
    "plt.title('Function estimation based on GB: test')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Разделяющие поверхности"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_classifier(model, X, y, ax=None, cmap='rainbow'):\n",
    "    ax = ax or plt.gca()\n",
    "    \n",
    "    # Plot the training points\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap=cmap,\n",
    "               clim=(y.min(), y.max()), zorder=3)\n",
    "    ax.axis('tight')\n",
    "    ax.axis('off')\n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = ax.get_ylim()\n",
    "    \n",
    "    # fit the estimator\n",
    "    model.fit(X, y)\n",
    "    xx, yy = np.meshgrid(np.linspace(*xlim, num=200),\n",
    "                         np.linspace(*ylim, num=200))\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "\n",
    "    # Create a color plot with the results\n",
    "    n_classes = len(np.unique(y))\n",
    "    contours = ax.contourf(xx, yy, Z, alpha=0.3,\n",
    "                           levels=np.arange(n_classes + 1) - 0.5,\n",
    "                           cmap=cmap,\n",
    "                           zorder=1)\n",
    "\n",
    "    ax.set(xlim=xlim, ylim=ylim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "X, y = make_blobs(n_samples=1000, centers=4,\n",
    "                  random_state=0, cluster_std=1.5)\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='rainbow');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf_r = DecisionTreeClassifier(max_depth=10)\n",
    "plt.figure(figsize=(20,10))\n",
    "visualize_classifier(clf_r, X, y, ax=None, cmap='rainbow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf_r = RandomForestClassifier(n_estimators=1000, max_depth=10)\n",
    "plt.figure(figsize=(20,10))\n",
    "visualize_classifier(clf_r, X, y, ax=None, cmap='rainbow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "clf_r = GradientBoostingClassifier(n_estimators=1000, learning_rate=1e-2, max_depth=3)\n",
    "plt.figure(figsize=(20,10))\n",
    "visualize_classifier(clf_r, X, y, ax=None, cmap='rainbow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Catboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для построения моделей градиентного бустинга можно использовать различные библиотеки - одними из самых популярных являются `xgboost`, `lightgbm`, `catboost`. Построим модель из `catboost` на основе данных о пассажирах с Титаника."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('titanic-train.csv')\n",
    "X = data.drop(['PassengerId', 'Survived', 'Name', 'Ticket', 'Cabin'], axis=1)\n",
    "y = data['Survived']\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Catboost спокойно справляется как с категориальными признаками, принимающими значения строк, так и с пропусками. Единственное - не справляется с пропусками в категориальных признаках :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[X.columns[X.dtypes == 'object']] = X[X.columns[X.dtypes == 'object']].fillna('NaN')\n",
    "X.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install catboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Изучите параметры в документации (https://catboost.ai/docs/concepts/python-reference_parameters-list.html) и подберите оптимальные с помощью кросс-валидации.\n",
    "\n",
    "https://catboost.ai/docs/concepts/parameter-tuning.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, roc_curve\n",
    "\n",
    "cb = CatBoostClassifier(\n",
    "    loss_function='Logloss',\n",
    "    cat_features=np.where(X_train.columns.isin(['Sex', 'Embarked']))[0], # categorical features processing\n",
    "    silent=True\n",
    ")\n",
    "\n",
    "param_grid = {\n",
    "    'max_depth': [1, 3, 5],\n",
    "    'n_estimators': [50, 150],\n",
    "    'learning_rate': [0.001, 0.01]\n",
    "}\n",
    "\n",
    "grid_cv = GridSearchCV(cb, param_grid, cv=3)\n",
    "grid_cv.fit(X_train, y_train)\n",
    "\n",
    "print('best cv accuracy score:', grid_cv.best_score_)\n",
    "\n",
    "cb_best = grid_cv.best_estimator_\n",
    "print('test accuracy score:', accuracy_score(y_test, cb_best.predict(X_test)))\n",
    "\n",
    "# feature importances\n",
    "fi = pd.Series(cb_best.feature_importances_, index=X_train.columns)\n",
    "fi.sort_values(ascending=False).plot(kind='bar')\n",
    "plt.title('feature importances')\n",
    "plt.show()\n",
    "\n",
    "# ROC curves\n",
    "plt.figure(figsize=[9, 6])\n",
    "fpr, tpr, _ = roc_curve(y_train, cb_best.predict_proba(X_train)[:, 1])\n",
    "plt.plot(fpr, tpr, 'r', label='train')\n",
    "fpr, tpr, _ = roc_curve(y_train, cb_best.predict(X_train))\n",
    "plt.plot(fpr, tpr, '--o', label='binary output, train')\n",
    "plt.legend(bbox_to_anchor=(0.999, 1))\n",
    "plt.title('ROC curves')\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
