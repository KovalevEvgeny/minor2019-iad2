{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Семинар 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Введение в нейронные сети"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/stack-more-layers.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## А зачем вообще это все?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/xor.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: CMU DL course"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](images/decision-boundaries.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](images/decision-boundaries2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## А как это обучать?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](images/backprop.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation\n",
    "\n",
    "* forward-шаг - проходим по сети от входов к выходам, запоминаем значения в нейронах\n",
    "* backward-шаг - проходим по сети от выходов к входам, считаем градиенты функции потерь по весам, обновляем веса\n",
    "\n",
    "и так далее..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Практика"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем работать с данными MNIST. Будем по картинке предсказывать наиболее вероятную цифру - на выходном слое сети будет десять нейронов с вероятностями, соответствующих цифрам."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = load_digits()\n",
    "X = digits.images / 255\n",
    "y = digits.target\n",
    "\n",
    "idx = np.random.randint(len(X), size=5)\n",
    "fig, ax = plt.subplots(1, 5, figsize=(16, 8))\n",
    "for i in range(5):\n",
    "    ax[i].imshow(X[idx[i]], cmap='gray')\n",
    "    ax[i].set_title(y[idx[i]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    \"\"\"\n",
    "    A building block. Each layer is capable of performing two things:\n",
    "    \n",
    "    - Process input to get output:           output = layer.forward(input)\n",
    "    \n",
    "    - Propagate gradients through itself:    grad_input = layer.backward(input, grad_output)\n",
    "    \n",
    "    Some layers also have learnable parameters which they update during layer.backward.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"Here you can initialize layer parameters (if any) and auxiliary stuff.\"\"\"\n",
    "        # A dummy layer does nothing\n",
    "        pass\n",
    "    \n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Takes input data of shape [1, input_units], returns output data [1, output_units]\n",
    "        \"\"\"\n",
    "        # A dummy layer just returns whatever it gets as input.\n",
    "        return input\n",
    "\n",
    "    def backward(self, input, grad_output):\n",
    "        \"\"\"\n",
    "        Performs a backpropagation step through the layer, with respect to the given input.\n",
    "        \n",
    "        To compute loss gradients w.r.t input, you need to apply chain rule (backprop):\n",
    "        \n",
    "        d loss / d x  = (d loss / d layer) * (d layer / d x)\n",
    "        \n",
    "        Luckily, you already receive d loss / d layer as input (grad output), so you only need to multiply it by d layer / d x.\n",
    "        \n",
    "        If your layer has parameters (e.g. dense layer), you also need to update them here using d loss / d layer\n",
    "        \"\"\"\n",
    "        # The gradient of a dummy layer is precisely grad_output, but we'll write it more explicitly\n",
    "        num_units = input.shape[1]\n",
    "        \n",
    "        d_layer_d_input = np.eye(num_units)\n",
    "        \n",
    "        return np.dot(grad_output, d_layer_d_input) # chain rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](images/activation-functions.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ReLU(Layer):\n",
    "    def __init__(self):\n",
    "        \"\"\"ReLU layer simply applies elementwise rectified linear unit to all inputs\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def forward(self, input):\n",
    "        \"\"\"Apply elementwise ReLU to [1, input_units] matrix\"\"\"\n",
    "        output = # ¯\\_(ツ)_/¯\n",
    "        return output\n",
    "    \n",
    "    def backward(self, input, grad_output):\n",
    "        \"\"\"Compute gradient of loss w.r.t. ReLU input\"\"\"\n",
    "        relu_grad = # ¯\\_(ツ)_/¯\n",
    "        return grad_output * relu_grad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Полносвязный слой"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](images/neural-network.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее для простоты будем считать, что в сеть приходит один объект (обычно это не так, и их приходить может сколько вы захотите).\n",
    "\n",
    "Предположим, нейрон на выходе один. Тогда если на входе в слой находился вектор $x \\in \\mathbb{R}^{1 \\times n}$, соответствующий признаковому описанию одного объекта, то на выходе будет:\n",
    "\n",
    "$$\n",
    "z = xw + b \\in \\mathbb{R},\n",
    "$$\n",
    "\n",
    "где $w \\in \\mathbb{R}^{n \\times 1}$ и $b \\in \\mathbb{R}$.\n",
    "\n",
    "А если нейронов на выходе несколько (k > 1)?\n",
    "\n",
    "$$\n",
    "z = xW + b \\in \\mathbb{R}^{1 \\times k},\n",
    "$$\n",
    "\n",
    "где $W \\in \\mathbb{R}^{n \\times k}$ и $b \\in \\mathbb{R}^{1 \\times k}$.\n",
    "\n",
    "Аккуратно посчитаем производные (для backward-шага). Нам нужно посчитать: $\\frac{\\partial L}{\\partial x}$, $\\frac{\\partial L}{\\partial W}$, $\\frac{\\partial L}{\\partial b}$.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial x} = \\frac{\\partial L}{\\partial z}\\frac{\\partial z}{\\partial x}\n",
    "$$\n",
    "\n",
    "Заметим, что $z_j = \\sum\\limits_i x_iw_{ij} + b_j$, поэтому $\\frac{\\partial z_j}{\\partial x_i} = w_{ij}$. Отсюда:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial z}{\\partial x} = \\begin{pmatrix}\\frac{\\partial z_1}{\\partial x_1}&\\cdots&\\frac{\\partial z_1}{\\partial x_n}\\\\\\vdots&\\ddots&\\vdots\\\\\\frac{\\partial z_k}{\\partial x_1}&\\cdots&\\frac{\\partial z_k}{\\partial x_n}\\end{pmatrix} = \\begin{pmatrix}w_{11}&\\cdots&w_{n1}\\\\\\vdots&\\ddots&\\vdots\\\\w_{1k}&\\cdots&w_{nk}\\end{pmatrix} = W^T\n",
    "$$\n",
    "\n",
    "Итак:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial x} = \\frac{\\partial L}{\\partial z}W^T\n",
    "$$\n",
    "\n",
    "Перейдем к производным по весам:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial w_{ij}} = \\frac{\\partial L}{\\partial z_j}\\frac{\\partial z_j}{\\partial w_{ij}} = \\frac{\\partial L}{\\partial z_j}x_i\n",
    "$$\n",
    "\n",
    "Отсюда:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W} = \\begin{pmatrix}\\frac{\\partial L}{\\partial w_{11}}&\\cdots&\\frac{\\partial L}{\\partial w_{1k}}\\\\\\vdots&\\ddots&\\vdots\\\\\\frac{\\partial L}{\\partial w_{n1}}&\\cdots&\\frac{\\partial L}{\\partial w_{nk}}\\end{pmatrix} = \\begin{pmatrix}\\frac{\\partial L}{\\partial z_1}x_1&\\cdots&\\frac{\\partial L}{\\partial z_k}x_1\\\\\\vdots&\\ddots&\\vdots\\\\\\frac{\\partial L}{\\partial z_1}x_n&\\cdots&\\frac{\\partial L}{\\partial z_k}x_n\\end{pmatrix} = \\begin{pmatrix}x_1\\\\\\vdots\\\\x_n\\end{pmatrix} \\begin{pmatrix}\\frac{\\partial L}{\\partial z_1}&\\cdots&\\frac{\\partial L}{\\partial z_k}\\end{pmatrix} = x^T\\frac{\\partial L}{\\partial z}\n",
    "$$\n",
    "\n",
    "Наконец, производная по bias:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial b_j} = \\frac{\\partial L}{\\partial z_j}\\frac{\\partial z_j}{\\partial b_j} = \\frac{\\partial L}{\\partial z_j}\n",
    "$$\n",
    "\n",
    "Следовательно:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial b} = \\frac{\\partial L}{\\partial z}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense(Layer):\n",
    "    def __init__(self, input_units, output_units, learning_rate=0.05):\n",
    "        \"\"\"\n",
    "        A dense layer is a layer which performs a learned affine transformation:\n",
    "        f(x) = <x*W> + b\n",
    "        \"\"\"\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # initialize weights with small random numbers. We use normal initialization, \n",
    "        # but surely there is something better. Try this once you got it working: http://bit.ly/2vTlmaJ\n",
    "        self.weights = np.random.randn(input_units, output_units)*0.01\n",
    "        self.biases = np.zeros((1, output_units))\n",
    "        \n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Perform an affine transformation:\n",
    "        f(x) = <x*W> + b\n",
    "        \n",
    "        input shape: [1, input_units]\n",
    "        output shape: [1, output units]\n",
    "        \"\"\"\n",
    "        return # ¯\\_(ツ)_/¯\n",
    "    \n",
    "    def backward(self, input, grad_output):\n",
    "        \n",
    "        # compute d f / d x = d f / d y * d y / d x\n",
    "        grad_input = # ¯\\_(ツ)_/¯\n",
    "        \n",
    "        # df / dw =  dy / dw * df / dy\n",
    "        # df / db =  dy / db * df / dy\n",
    "        \n",
    "        # compute gradient w.r.t. weights and biases\n",
    "        grad_weights = # ¯\\_(ツ)_/¯\n",
    "        grad_biases = # ¯\\_(ツ)_/¯\n",
    "        \n",
    "        assert grad_weights.shape == self.weights.shape and grad_biases.shape == self.biases.shape\n",
    "        # Here we perform a stochastic gradient descent step. \n",
    "        # Later on, you can try replacing that with something better.\n",
    "        self.weights = self.weights - self.learning_rate * grad_weights\n",
    "        self.biases = self.biases - self.learning_rate * grad_biases\n",
    "        \n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Функция потерь\n",
    "\n",
    "Кросс-энтропия:\n",
    "\n",
    "$$\n",
    "L(y, p) = -\\sum_j y_j \\log p_j = -[y_i = 1] \\log p_i\n",
    "$$\n",
    "\n",
    "Заметим, что для вычисления вероятностей в последнем слое в качестве функции активации используется софтмакс:\n",
    "\n",
    "$$\n",
    "p_i = \\frac{e^{x_i}}{\\sum\\limits_{j}e^{x_j}}\n",
    "$$\n",
    "\n",
    "Подставим это в функцию потерь:\n",
    "\n",
    "$$\n",
    "L(y, p) = -[y_i = 1] \\log \\frac{e^{x_i}}{\\sum\\limits_{j}e^{x_j}} = -[y_i = 1]x_i + \\log\\sum\\limits_{j}e^{x_j}\n",
    "$$\n",
    "\n",
    "Эта функция потерь называется log-softmax и часто используется вместо того, чтобы посчитать вероятности и затем посчитать кросс-энтропию, в силу ее численных особенностей.\n",
    "\n",
    "Нетрудно посчитать ее производную:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial x} = -y_{oh} + \\frac{e^x}{\\sum_j e^{x_j}},\n",
    "$$\n",
    "\n",
    "где $y_{oh}$ - one-hot вектор $(0, ..., y_i = 1, ..., 0)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax_crossentropy_with_logits(logits, answer):\n",
    "    \"\"\"Compute crossentropy from logits[1, n_classes] and ids of correct answers\"\"\"\n",
    "    logits_for_answers = logits[0, answer]\n",
    "    \n",
    "    xentropy = -logits_for_answers + np.log(np.exp(logits).sum())\n",
    "    \n",
    "    return xentropy\n",
    "\n",
    "def grad_softmax_crossentropy_with_logits(logits, answer):\n",
    "    \"\"\"Compute crossentropy gradient from logits[1, n_classes] and ids of correct answers\"\"\"\n",
    "    ones_for_answers = np.zeros_like(logits)\n",
    "    ones_for_answers[0, answer] = 1\n",
    "    \n",
    "    softmax = np.exp(logits) / np.exp(logits).sum()\n",
    "    \n",
    "    return -ones_for_answers + softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим архитектуру сети."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# векторизуем картинки\n",
    "X_flat = X.reshape(X.shape[0], -1)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_flat, y, test_size=0.25, random_state=13)\n",
    "\n",
    "network = []\n",
    "network.append(Dense(X_train.shape[1], 32))\n",
    "network.append(ReLU())\n",
    "network.append(Dense(32, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функции для обучения сети и предсказания."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward(network, X):\n",
    "    \"\"\"\n",
    "    Compute activations of all network layers by applying them sequentially.\n",
    "    Return a list of activations for each layer. \n",
    "    Make sure last activation corresponds to network logits.\n",
    "    \"\"\"\n",
    "    activations = []\n",
    "    input = X.copy()\n",
    "    for layer in network:\n",
    "        input = layer.forward(input)\n",
    "        activations += [input]\n",
    "    \n",
    "    assert len(activations) == len(network)\n",
    "    return activations\n",
    "\n",
    "def predict(network, X):\n",
    "    \"\"\"\n",
    "    Use network to predict the most likely class for each sample.\n",
    "    \"\"\"\n",
    "    logits = forward(network, X)[-1]\n",
    "    return logits.argmax(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(network,X,y):\n",
    "    \"\"\"\n",
    "    Train your network on a given batch of X and y.\n",
    "    You first need to run forward to get all layer activations.\n",
    "    You can estimate loss and loss_grad, obtaining dL / dy_pred\n",
    "    Then you can run layer.backward going from last layer to first, \n",
    "    propagating the gradient of input to previous layers.\n",
    "    \n",
    "    After you called backward for all layers, all Dense layers have already made one gradient step.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the layer activations\n",
    "    layer_activations = forward(network,X)\n",
    "    layer_inputs = [X] + layer_activations  #layer_input[i] is an input for network[i]\n",
    "    logits = layer_activations[-1]\n",
    "    \n",
    "    # Compute the loss and the initial gradient\n",
    "    loss = softmax_crossentropy_with_logits(logits,y)\n",
    "    loss_grad = grad_softmax_crossentropy_with_logits(logits,y)\n",
    "    \n",
    "    # propagate gradients through network layers using .backward\n",
    "    # hint: start from last layer and move to earlier layers\n",
    "    grad_output = loss_grad\n",
    "    for layer, layer_input in zip(reversed(network), reversed(layer_inputs[:-1])):\n",
    "        grad_output = layer.backward(layer_input, grad_output)\n",
    "    return np.mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def iterate_minibatches(inputs, targets, shuffle=False):\n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.random.permutation(len(inputs))\n",
    "    for batch_idx in range(int(len(inputs))):\n",
    "        if shuffle:\n",
    "            excerpt = indices[batch_idx]\n",
    "        else:\n",
    "            excerpt = batch_idx\n",
    "        yield inputs[excerpt].reshape(1, -1), targets[excerpt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "train_log = []\n",
    "val_log = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(100):\n",
    "\n",
    "    for x_batch, y_batch in iterate_minibatches(X_train, y_train, shuffle=True):\n",
    "        train(network, x_batch, y_batch)\n",
    "    \n",
    "    train_correct = 0\n",
    "    for x_batch, y_batch in iterate_minibatches(X_train, y_train, shuffle=False):\n",
    "        y_pred = predict(network, x_batch)\n",
    "        if y_pred == y_batch:\n",
    "            train_correct += 1\n",
    "    \n",
    "    val_correct = 0\n",
    "    for x_batch, y_batch in iterate_minibatches(X_val, y_val, shuffle=False):\n",
    "        y_pred = predict(network, x_batch)\n",
    "        if y_pred == y_batch:\n",
    "            val_correct += 1\n",
    "\n",
    "    train_log.append(train_correct / len(y_train))\n",
    "    val_log.append(val_correct / len(y_val))\n",
    "    \n",
    "    clear_output()\n",
    "    print(\"Epoch\",epoch)\n",
    "    print(\"Train accuracy:\",train_log[-1])\n",
    "    print(\"Val accuracy:\",val_log[-1])\n",
    "    plt.plot(train_log,label='train accuracy')\n",
    "    plt.plot(val_log,label='val accuracy')\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.random.randint(len(X_val), size=8)\n",
    "fig, ax = plt.subplots(1, 8, figsize=(16, 8))\n",
    "for i in range(8):\n",
    "    x_batch = X_val[idx[i]]\n",
    "    ax[i].imshow(x_batch.reshape(8, 8), cmap='gray')\n",
    "    ax[i].set_title(predict(network, x_batch)[0])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
